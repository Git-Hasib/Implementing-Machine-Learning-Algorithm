{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Decision Tree.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNG0hoRO7iYX5DcpNVsIzZR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"c35BPiX2inrs"},"source":["**Importing Libraries**"]},{"cell_type":"code","metadata":{"id":"7qxt8eTKikZF"},"source":["import numpy as np\n","import math\n","import random\n","import pandas as pd\n","from collections import Counter\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ra-hTadinCU"},"source":["**Dataset Loading and Preprocessing**"]},{"cell_type":"code","metadata":{"id":"k6mASBtWimXc"},"source":["df = pd.read_csv(\"WA_Fn_UseC_Telco_Customer_Churn.csv\",\n","                 na_values=[\"No internet service\",\"No phone service\"])\n","\n","df.dropna(inplace=True) #Dropping Rows with NaN\n","df.dropna(inplace=True, axis='columns') #Dropping Columns with NaN\n","df.reset_index(drop=True)\n","\n","\n","df = df.replace({\n","    \"Churn\": {\n","        \"Yes\" : 1,\n","        \"No\" : 0\n","    }\n","})\n","\n","#Dropping the Unnecessary Columns\n","df.drop(labels=[\"customerID\", \"Partner\", \\\n","                \"StreamingTV\", \"PhoneService\"], \n","        axis=1,\n","        inplace=True)\n","\n","#Identifying the Categorical and Numerical Columns\n","categorical_data = pd.DataFrame(df, columns = [\"gender\", \"Dependents\", \n","                                                  \"MultipleLines\", \n","                                                  \"InternetService\",\n","                                                  \"OnlineSecurity\", \n","                                                  \"OnlineBackup\", \n","                                                  \"DeviceProtection\", \n","                                                  \"TechSupport\", \n","                                                  \"StreamingMovies\", \n","                                                  \"Contract\", \n","                                                  \"PaperlessBilling\", \n","                                                  \"PaymentMethod\", \"Churn\"])\n","numerical_data = pd.DataFrame( df, columns = [\"SeniorCitizen\", \"tenure\",\n","                                                \"TotalCharges\", \n","                                                \"MonthlyCharges\"])   \n","\n","#defining feature matrix and response vector\n","data_x = categorical_data.loc[:, categorical_data.columns != 'Churn'] \n","data_y = df[\"Churn\"]\n","\n","#Dropping the Numerical Columns\n","numerical_data.drop(labels=numerical_data, \n","        axis=1,\n","        inplace=True)\n","\n","#Converting dataframe into numpy array\n","categorical_data = np.asarray(categorical_data)\n","\n","\n","#Split the dataset (80% training, 20% testing) with stratification (using random_state = 911)\n","\n","train, test = train_test_split(categorical_data, \n","                                train_size = 0.8, \n","                                stratify = data_y,\n","                                random_state=911)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJPiknl1Ugjs"},"source":["**Train and Test set**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0O-6X3Et1qc","executionInfo":{"status":"ok","timestamp":1637646513856,"user_tz":-360,"elapsed":5,"user":{"displayName":"Hasibul Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-8qyn-Y6WG3owqvn9wfRwmqADwwBkHnW_Bvz6g=s64","userId":"00451471846498296598"}},"outputId":"35b81365-9a97-4323-d2d0-fc8eb4b4c9ed"},"source":["print(\"Train set: {}\".format(train.shape))\n","print(train[: 5])\n","\n","print(\"-------------------------------------------\")\n","print(\"Test set: {}\".format(test.shape))\n","print(test[: 5])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set: (3868, 13)\n","[['Male' 'No' 'No' 'Fiber optic' 'No' 'No' 'No' 'No' 'Yes'\n","  'Month-to-month' 'Yes' 'Electronic check' 1]\n"," ['Male' 'No' 'Yes' 'Fiber optic' 'No' 'No' 'Yes' 'No' 'Yes'\n","  'Month-to-month' 'Yes' 'Electronic check' 0]\n"," ['Female' 'No' 'No' 'DSL' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Two year' 'No'\n","  'Credit card (automatic)' 0]\n"," ['Female' 'No' 'Yes' 'DSL' 'Yes' 'No' 'Yes' 'Yes' 'Yes' 'Two year' 'No'\n","  'Bank transfer (automatic)' 0]\n"," ['Male' 'Yes' 'Yes' 'DSL' 'Yes' 'Yes' 'Yes' 'Yes' 'Yes' 'Two year' 'No'\n","  'Mailed check' 0]]\n","-------------------------------------------\n","Test set: (967, 13)\n","[['Female' 'No' 'Yes' 'Fiber optic' 'No' 'Yes' 'Yes' 'No' 'Yes'\n","  'One year' 'Yes' 'Bank transfer (automatic)' 0]\n"," ['Female' 'No' 'No' 'Fiber optic' 'No' 'No' 'No' 'No' 'No'\n","  'Month-to-month' 'Yes' 'Electronic check' 1]\n"," ['Female' 'No' 'No' 'Fiber optic' 'Yes' 'Yes' 'Yes' 'Yes' 'Yes'\n","  'Month-to-month' 'Yes' 'Bank transfer (automatic)' 1]\n"," ['Female' 'No' 'No' 'Fiber optic' 'No' 'No' 'Yes' 'No' 'Yes'\n","  'Month-to-month' 'No' 'Electronic check' 1]\n"," ['Female' 'No' 'Yes' 'DSL' 'Yes' 'Yes' 'No' 'Yes' 'No' 'One year' 'Yes'\n","  'Mailed check' 1]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"JB6WdI3hkFt1"},"source":["**Working On Skeleton Code**"]},{"cell_type":"code","metadata":{"id":"HjcTqoUOZJBl"},"source":["class Node:\n","    def __init__(self, attribute=None, attribute_values=None, \n","                 child_nodes=None, decision=None):\n","        self.attribute = attribute\n","        self.attribute_values = attribute_values\n","        self.child_nodes = child_nodes\n","        self.decision = decision\n","\n","\n","class DecisionTree:\n","\n","    root = None\n","\n","    @staticmethod\n","    def plurality_values(data):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        return Counter(labels).most_common(1)[0][0] #returns -> list of tuple\n","\n","    @staticmethod\n","    def all_zero(data):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        return np.all(labels == 0)\n","\n","    @staticmethod\n","    def all_one(data):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        return np.all(labels == 1)\n","\n","    @staticmethod\n","    def importance(data, attributes):\n","        labels = data[:, data.shape[1] - 1]  # store the last column in labels\n","        unique_element, frequency = np.unique(labels, return_counts=True)\n","        pos = 0\n","        neg = 0\n","\n","        if len(unique_element) == 1:\n","            if unique_element[0] == '0':\n","                pos = 0\n","                neg = int(frequency[0])\n","            elif unique_element[0] == '1':\n","                pos = int(frequency[0])\n","                neg = 0\n","        else:\n","            neg = int(frequency[0])\n","            pos = int(frequency[1]) \n","\n","        gain = 0 # entropy of parent node\n","        entropy_child = 0\n","        if neg == 0:\n","            gain = -pos*math.log2(pos)\n","        elif pos == 0:\n","            gain = -neg*math.log2(neg)\n","        else:\n","            gain = -neg*math.log2(neg) - pos*math.log2(pos)\n","\n","        max_attr = -1\n","        max_gain = -math.inf\n","        \n","        for attr in attributes:\n","            unique_values = np.unique(data[:, attr])\n","            for val in unique_values:        \n","                no_k = pos_k = 0\n","                for i in range(data.shape[0]):\n","                    if labels[i] == 0:\n","                        no_k += 1\n","                    elif labels[i] == 1:\n","                        pos_k += 1\n","                \n","                if pos_k == 0:\n","                    entropy_child = -(no_k/(pos_k+no_k)) * math.log2(no_k/(pos_k+no_k))   \n","                elif no_k == 0:\n","                    entropy_child = -(pos_k/(pos_k+no_k)) * math.log2(pos_k/(pos_k+no_k))\n","                else:\n","                    child_entropy = (-(pos_k/(pos_k+no_k)) * \n","                                     math.log2(pos_k/(pos_k+no_k)) - (no_k/(pos_k+no_k)) \n","                                     * math.log2(no_k/(pos_k+no_k)))\n","\n","                gain -= ((pos_k + no_k) / (neg + pos)) * entropy_child        \n","          \n","            if  gain > max_gain:\n","                max_gain = gain\n","                max_attr = attr    \n","        \n","        return max_attr\n","\n","\n","    def train(self, data, attributes, parent_data):\n","        data = np.array(data)\n","        parent_data = np.array(parent_data)\n","        attributes = list(attributes)\n","        \n","        if data.shape[0] == 0:  # if x is empty\n","            return Node(decision=self.plurality_values(parent_data))\n","\n","        elif self.all_zero(data):\n","            return Node(decision=0)\n","\n","        elif self.all_one(data):\n","            return Node(decision=1)\n","\n","        elif len(attributes) == 0:\n","            return Node(decision=self.plurality_values(data))\n","\n","        else:\n","            a = self.importance(data, attributes)\n","            tree = Node(attribute=a, attribute_values=np.unique(data[:, a]), child_nodes=[])\n","            attributes.remove(a)\n","            for vk in np.unique(data[:, a]):\n","                new_data = data[data[:, a] == vk, :]\n","                subtree = self.train(new_data, attributes, data)\n","                tree.child_nodes.append(subtree)\n","\n","            return tree\n","\n","    def fit(self, data):\n","        self.root = self.train(data, list(range(data.shape[1] - 1)), np.array([]))\n","        \n","    def predict(self, data):\n","        predictions = []\n","        for i in range(data.shape[0]):\n","            current_node = self.root\n","            while True:\n","                if current_node.decision is None:\n","                    current_attribute = current_node.attribute\n","                    current_attribute_value = data[i, current_attribute]\n","                    if current_attribute_value not in current_node.attribute_values:\n","                        predictions.append(random.randint(0, 1))\n","                        break\n","                    idx = list(current_node.attribute_values).index(current_attribute_value)\n","\n","                    current_node = current_node.child_nodes[idx]\n","                else:\n","                    predictions.append(current_node.decision)\n","                    break\n","\n","        return predictions\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKEphOtFwFYZ"},"source":["dc_tree = DecisionTree()\n","dc_tree.fit(train)\n","prediction = dc_tree.predict(test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vmwyx9iURDc"},"source":["**Generating Confusion Matrix**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-t5Ao1WOL61","executionInfo":{"status":"ok","timestamp":1637646514739,"user_tz":-360,"elapsed":8,"user":{"displayName":"Hasibul Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-8qyn-Y6WG3owqvn9wfRwmqADwwBkHnW_Bvz6g=s64","userId":"00451471846498296598"}},"outputId":"d6d3c2f4-d612-41f3-b8bd-73f0bb7f96ab"},"source":["def confusion_matrix(decesion_list, y_test):\n","    y_test = np.array(y_test)\n","    \n","    tp = tn = fp = fn = 0\n","    for i in range(len(decesion_list)):\n","        if decesion_list[i] == 1 and y_test[i] == 1:\n","            tp += 1\n","        elif decesion_list[i] == 0 and y_test[i] == 0:\n","            tn += 1\n","        elif decesion_list[i] == 0 and y_test[i] == 1:\n","            fn += 1 \n","        elif decesion_list[i] == 1 and y_test[i] == 0:\n","            fp += 1\n","    return tp, tn, fn, fp\n","\n","tp, tn, fn, fp = confusion_matrix(prediction, test[:, -1]) \n","\n","print(\"TP = {}, TN = {}, FP = {}, FN = {}\".format(tp, tn, fp, fn))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TP = 175, TN = 451, FP = 199, FN = 142\n"]}]},{"cell_type":"code","metadata":{"id":"flVDpCuiObxd"},"source":["def precision_score(tp, tn, fn, fp):\n","    return ( tp / (fp + tp) )\n","def accuracy_score(tp, tn, fn, fp):\n","    return  (tp + tn)/ (tp + fn + tn + fp)\n","def recall_score(tp, tn, fn, fp):\n","    return tp / (fn + tp)\n","def f1_score(tp, tn, fn, fp):\n","    return (2* precision_score(tp, tn, fn, fp) * recall_score(tp, tn, fn, fp) \n","            / (precision_score(tp, tn, fn, fp) + recall_score(tp, tn, fn, fp)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-6qVKR29TzRt"},"source":["**Performance Measurement Report**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0abfJgTT6Hb","executionInfo":{"status":"ok","timestamp":1637646514740,"user_tz":-360,"elapsed":6,"user":{"displayName":"Hasibul Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-8qyn-Y6WG3owqvn9wfRwmqADwwBkHnW_Bvz6g=s64","userId":"00451471846498296598"}},"outputId":"4a2cab4a-23cf-4faf-fc7d-82cabf18f2da"},"source":["prec = precision_score(tp, tn, fn, fp)\n","acc = accuracy_score(tp, tn, fn, fp)\n","recall = recall_score(tp, tn, fn, fp)\n","f1 = f1_score(tp, tn, fn, fp)\n","\n","print(\"Precision Score = {:10.6f}\".format(prec))    \n","print(\"Accuracy Score = {:11.6f}\".format(acc))    \n","print(\"Recall Score = {:13.6f}\".format(recall))    \n","print(\"F-1 Score = {:16.6f}\".format(f1))  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision Score =   0.467914\n","Accuracy Score =    0.647363\n","Recall Score =      0.552050\n","F-1 Score =         0.506512\n"]}]}]}