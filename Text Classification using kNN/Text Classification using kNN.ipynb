{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Classification using kNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOra4gCMONH19LaOh+cfNAt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wf3YCx7wVVrj","executionInfo":{"status":"ok","timestamp":1640986886865,"user_tz":-360,"elapsed":422,"user":{"displayName":"Hasibul Hasan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjg-8qyn-Y6WG3owqvn9wfRwmqADwwBkHnW_Bvz6g=s64","userId":"00451471846498296598"}},"outputId":"6fff23ce-7d11-4a71-c734-011262a019f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import math\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","\n","import string\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","# from bs4 import BeautifulSoup as bs\n","# import lxml\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import math\n","import enum\n"]},{"cell_type":"code","source":["class Metric(enum.Enum):\n","    EUCLIDEAN_DISTANCE = \"euclidean\"\n","    HAMMING_DISTANCE = \"hamming\"\n","    MANHATTAN_DISTANCE = \"manhattan\"\n","    COSINE_SIMILARITY = \"cosine\""],"metadata":{"id":"dHPRCa5_WuXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.util import clean_html\n","class kNN_Texts:\n","\n","    ### DO NOT change anything in the constructor\n","    def __init__(self, vectorizer_class=None, \n","                 K=None,\n","                 metric=Metric.EUCLIDEAN_DISTANCE):\n","        \n","        self.__vectorizer = vectorizer_class(analyzer=lambda text: text)\n","        # self.__vectorizer = vectorizer_class(analyzer=self.__preprocess_text)\n","        \n","        self.__K = K\n","        self.__metric = metric\n","\n","        self.__train_vocabulary = None\n","        self.__train_feature_vectors = None\n","        self.__train_labels = None\n","\n","    \"\"\"@staticmethod\"\"\"\n","    def __preprocess_text(self, text):\n","        ### TODO \n","        #Lowercase the text\n","        text = text.lower() \n","        #Number Removal\n","        text = re.sub(r'[-+]?\\d+', '', text)  \n","        \n","        #Remove hyperlinks\n","        text = re.sub(r'https?:\\/\\/\\S*', '', text)\n","        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","        text = re.sub(r'https?:\\/\\/.*\\s*', '', text)\n","        text = re.sub(r'www\\.\\S*', '', text)\n","        text = re.sub(r'\\S*\\.(com|info|net|org)', '', text)\n","        #Remove punctuations\n","        text = text.translate((str.maketrans('', '', string.punctuation)))  \n","        #Tokenize\n","        text = word_tokenize(text)\n","        #Remove stopwords\n","        stop_words = set(stopwords.words('english'))\n","        text = [word for word in text if not word in stop_words]\n","        #Lemmatize tokens\n","        lemmatizer = WordNetLemmatizer()\n","        text = [lemmatizer.lemmatize(word) for word in text]\n","        #Stemming tokensaazwxx\n","        stemmer = PorterStemmer()\n","        text = [stemmer.stem(word) for word in text]\n","\n","        preprocessed_text = text\n","\n","        return preprocessed_text\n","\n","    def __fit_vectorizer(self, texts):\n","        ### TODO\n","        # fit the attribute vectorizer\n","        self.__vectorizer.fit(texts)  \n","        ### TODO\n","        self.__train_vocabulary = self.__vectorizer.vocabulary_  #\n","\n","    def __vectorize_texts(self, texts):\n","        ### TODO\n","        texts_feature_vectors = self.__vectorizer.transform(texts).toarray()\n","\n","        return texts_feature_vectors\n","\n","    def __train(self, texts, labels):\n","        ### TODO\n","        preprocessed_texts = list(map(self.__preprocess_text, texts))\n","        self.__fit_vectorizer(preprocessed_texts)\n","        ### TODO\n","        train_feature_vectors = self.__vectorize_texts(preprocessed_texts)\n","       \n","        ### TODO\n","        train_labels = list(labels)  \n","       \n","        return train_feature_vectors, train_labels\n","\n","    def fit(self, texts, labels):\n","        self.__train_feature_vectors, self.__train_labels = self.__train(texts=texts, \n","                                                                         labels=labels)\n","\n","    def __compute_metric_to_train_points(self, feature_vector):\n","        metric_values = None\n","\n","        ### TODO\n","        a = feature_vector\n","        t = self.__train_feature_vectors\n","\n","        if self.__metric == Metric.EUCLIDEAN_DISTANCE:\n","            #Eucl_dist = sqrt ( sum( (a[i]-b[i])^2) )\n","            dist = []\n","            for i in range(len(t)):  # For each feature in train features\n","                b = t[i]\n","                sum = 0\n","                for j in range(len(b)):\n","                    sum += pow((a[j] - b[j]), 2)\n","                sqrt = math.sqrt(sum)    \n","                \n","                dist.append((sqrt, self.__train_labels[i]))\n","            metric_values = dist\n","        \n","        elif self.__metric == Metric.HAMMING_DISTANCE:\n","            # Hd = count++ if a[j] != b[j]\n","            dist = []\n","            for i in range(len(t)):  # For each feature in train features\n","                b = t[i]\n","                sum = 0\n","                for j in range(len(b)):\n","                    if a[j] != b[j]:\n","                        sum += 1   \n","                dist.append((sum, self.__train_labels[i]))\n","            metric_values = dist\n","\n","        elif self.__metric == Metric.MANHATTAN_DISTANCE:\n","            #MH_dist = sum( abs(a[i]-b[i]) )\n","            dist = []\n","            for i in range(len(t)):  \n","                b = t[i]\n","                sum = 0\n","                for j in range(len(b)):\n","                    sum += abs(a[j] - b[j])    \n","                dist.append((sum, self.__train_labels[i]))\n","\n","            metric_values = dist\n","\n","        elif self.__metric == Metric.COSINE_SIMILARITY:\n","            #cos_sim = Sum_of(a[i].b[i]) / sqrt( sum_of(a[i]^2) ) . sqrt( sum_of(b[i]^2) )\n","            dist = []\n","            for i in range(len(t)):     # For each feature in the train features\n","                b = t[i]                # a = A Feature Vector,  b = A single Train Feature Vector, \n","                sum, aj_bj, a_sqr, b_sqr = 0\n","                for j in range(len(b)):\n","                    aj_bj += (a[j] * b[j])\n","                    a_sqr += pow(a[i], 2)\n","                    b_sqr += pow(b[i], 2)\n","                \n","                res = aj_bj / (math.sqrt(a_sqr) * math.sqrt(b_sqr))\n","                dist.append((res, self.__train_labels[i]))\n","            metric_values = dist\n","\n","        return metric_values\n","\n","    def predict(self, texts):\n","        ### TODO\n","        preprocessed_texts = list(map(self.__preprocess_text, texts))\n","\n","        ### TODO\n","        test_feature_vectors = self.__vectorize_texts(preprocessed_texts)\n","\n","        ### TODO \n","        predict_label = []\n","        for i in range(len(test_feature_vectors)):\n","            distance_metric = self.__compute_metric_to_train_points(test_feature_vectors[i])\n","            \n","            flag = 1 #For \"cosine_similarity\", make the flag = 0 \n","            if flag == 0:\n","                #for \"cosine_similarity\"\n","                distance_metric.sort(reverse = True)  #Descendingly\n","            else:\n","                distance_metric.sort()  #Ascendingly\n","            \n","            distance_metric = distance_metric[: self.__K]\n","            c_1 = 0; c_0 = 0\n","            for d in distance_metric:\n","                if d[1] == 1:\n","                    c_1 += 1\n","                elif d[1] == 0:\n","                    c_0 += 1  \n","                else:\n","                    pass      \n","            \n","            if c_1 > c_0:\n","                major_class = 1\n","            else:\n","                major_class = 0      \n","            \n","            predict_label.append(major_class)     \n","            \n","\n","            waighted = False #make 'True' if waighted kNN is need \n","            if waighted == True :\n","                w_prediction = 0\n","                for d in distance_metric:\n","                    if self.__train_labels[i] == 0:\n","                        w_prediction += (1/d[0]) * -1\n","                    else:\n","                        w_prediction += (1/d[0]) * 1\n","            \n","                predict_label.append(1 if w_prediction >= 0 else 0)\n","\n","        #test_metric_values = None\n","       \n","        ### TODO\n","        predictions = predict_label\n","\n","        return predictions"],"metadata":{"id":"4a7DErOvWz2r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train.csv**"],"metadata":{"id":"p68h1zTVhzSm"}},{"cell_type":"code","source":["\n","train_set = pd.read_csv(\"train.csv\", na_values='')\n","\n","train_set.drop(columns = 'id', axis = 1)\n","train_set.dropna(inplace = True)\n","train_set.dropna()\n","train_set.reset_index(inplace = True)\n","\n","test_set = pd.read_csv(\"test.csv\", na_values='')\n","test_set.dropna(inplace = True)\n","test_set.reset_index(inplace = True)\n","test_set.drop(columns = 'id', axis = 1)\n","\n","train_label = train_set[\"label\"]\n","train_feature = train_set[\"tweet\"]\n","\n","x_train, x_test, y_train, y_test= train_test_split(train_feature, train_label, \n","                                   train_size = 0.8, \n","                                   stratify = train_label,\n","                                   random_state = 911 )  #With stratify\n","                                                                                              "],"metadata":{"id":"BHOFVtU0XDh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**CountVectorizer**"],"metadata":{"id":"kkYZqZrbBIy_"}},{"cell_type":"code","source":["clf = kNN_Texts(vectorizer_class=CountVectorizer,\n","                metric=Metric.EUCLIDEAN_DISTANCE,\n","                K=9)\n","\n","clf.fit(x_train, y_train)\n","predictions = clf.predict(x_test)\n","print(len(y_test))\n","print(len(predictions))\n","\n","\n","tn, fp, fn, tp = confusion_matrix(predictions, y_test)\n","\n","print(\"TP: {}\\nTN: {}\\nFP: {}\\nFN: {}\".format(tp, tn, fp, fn))\n","\n","\n","print(clf.predict(test_set))"],"metadata":{"id":"lKtEyn93W9y8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TfidfVectorizer**"],"metadata":{"id":"SinAS2veBOxJ"}},{"cell_type":"code","source":["clf = kNN_Texts(vectorizer_class=TfidfVectorizer,\n","                metric=Metric.EUCLIDEAN_DISTANCE,\n","                K=9)\n","\n","clf.fit(x_train, y_train)\n","predictions = clf.predict(x_test)\n","print(len(y_test))\n","print(len(predictions))\n","\n","\n","tn, fp, fn, tp = confusion_matrix(predictions, y_test)\n","\n","print(\"TP: {}\\nTN: {}\\nFP: {}\\nFN: {}\".format(tp, tn, fp, fn))\n","\n","\n","print(clf.predict(test_set))"],"metadata":{"id":"dJZEPQ30kiqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf._kNN_Texts__metric"],"metadata":{"id":"1V_kEf0tXAMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir(clf)"],"metadata":{"id":"vyM0W8hoXCaE"},"execution_count":null,"outputs":[]}]}