{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHdUusQEDyDC",
        "outputId": "1b273fb1-3b52-44f9-c9ed-022dba1c6cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "# from bs4 import BeautifulSoup as bs\n",
        "# import lxml\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    #Original text\n",
        "    print(\"Original:\")\n",
        "    print(text)\n",
        "    print()\n",
        " \n",
        "    #Lowercase the text\n",
        "    text = text.lower()\n",
        "    print(\"Lowercased:\")\n",
        "    print(text)\n",
        "    print()\n",
        " \n",
        "    #Number Removal\n",
        "    text = re.sub(r'[-+]?\\d+', '', text)\n",
        "    print(\"Integer Numbers removed:\")\n",
        "    print(text)\n",
        "    print()\n",
        " \n",
        "    #Remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/\\S*', '', text)\n",
        "    #text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    #text = re.sub(r'https?:\\/\\/.*\\s*', '', text)\n",
        "    text = re.sub(r'www\\.\\S*', '', text)\n",
        "    text = re.sub(r'\\S*\\.(com|info|net|org)', '', text)\n",
        "    print(\"Sample Hyperlinks removed:\")\n",
        "    print(text)\n",
        "    print()\n",
        " \n",
        "    #Remove punctuations\n",
        "    text = text.translate((str.maketrans('', '', string.punctuation)))\n",
        "    print(\"Sample Punctuations removed:\")\n",
        "    print(text)\n",
        "    print()\n",
        " \n",
        "    #Tokenize\n",
        "    text = word_tokenize(text)\n",
        "    print(\"Tokenized(word-level):\")\n",
        "    print(text)\n",
        "    print()\n",
        " \n",
        "    #Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    print(\"Sample Stopwords removed:\")\n",
        "    print(text)\n",
        "    print()\n",
        "\n",
        "    #Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    print(\"Tokens Lemmatized:\")\n",
        "    print(text)\n",
        "    print()\n",
        "\n",
        "    #Stemming tokens\n",
        "    stemmer = PorterStemmer()\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    print(\"Tokens Stemmed:\")\n",
        "    print(text)\n",
        "    print()\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "Qa7FmpVrEA8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "         \"You shall not pass.\",\n",
        "         \"The axe forgets but the tree remembers.\",\n",
        "         \"What is dead may never die.\",\n",
        "         \"Thou shall not cheat.\",\n",
        "         \"Everybody lies.\",\n",
        "         \"The lone wolf dies, but the pack survives.\"\n",
        "]"
      ],
      "metadata": {
        "id": "WlKczAwwEEd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_texts = list(map(preprocess_text, texts))"
      ],
      "metadata": {
        "id": "YRRqpp8GEHDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_texts"
      ],
      "metadata": {
        "id": "6yChWOizELbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "ic-H1ojCENtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_vectorizer(vectorizer_class, texts):\n",
        "  print(f\"Vectorizing with {vectorizer_class}\")\n",
        "  print()\n",
        "\n",
        "  texts_vectorizer = vectorizer_class(analyzer=lambda text: text)\n",
        "  texts_vectorizer.fit(texts)\n",
        "    \n",
        "  # Printing the identified Unique words along with their indices\n",
        "  print(\"Vocabulary:\")\n",
        "  print(texts_vectorizer.vocabulary_)\n",
        "  print()\n",
        "\n",
        "  return texts_vectorizer\n",
        "\n",
        "def vectorize_texts(texts_vectorizer, texts):\n",
        "\n",
        "  # Encode the Document\n",
        "  texts_vectors = texts_vectorizer.transform(texts).toarray()\n",
        "    \n",
        "  # Summarizing the Encoded Texts\n",
        "  print(\"Encoded Documents:\")\n",
        "  print(texts_vectors)\n",
        "\n",
        "  return texts_vectors"
      ],
      "metadata": {
        "id": "QwMlwWPGEQ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = fit_vectorizer(CountVectorizer, texts=preprocessed_texts)\n",
        "vectorize_texts(vectorizer, texts=preprocessed_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXGTB2mbEUlf",
        "outputId": "8b1803f5-69c8-4568-d6f7-431267107330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizing with <class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
            "\n",
            "Vocabulary:\n",
            "{'shall': 14, 'pa': 11, 'axe': 0, 'forget': 6, 'tree': 17, 'rememb': 13, 'dead': 2, 'may': 9, 'never': 10, 'die': 3, 'thou': 16, 'cheat': 1, 'everybodi': 5, 'lie': 7, 'lone': 8, 'wolf': 18, 'dy': 4, 'pack': 12, 'surviv': 15}\n",
            "\n",
            "Encoded Documents:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            " [1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0]\n",
            " [0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_texts(vectorizer, texts=[preprocess_text(\"Everybody dies some day.\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVGJGTSeEWcC",
        "outputId": "0d2c6c69-dbed-4d2c-eb8c-06dbecda1b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "Everybody dies some day.\n",
            "\n",
            "Lowercased:\n",
            "everybody dies some day.\n",
            "\n",
            "Integer Numbers removed:\n",
            "everybody dies some day.\n",
            "\n",
            "Sample Hyperlinks removed:\n",
            "everybody dies some day.\n",
            "\n",
            "Sample Punctuations removed:\n",
            "everybody dies some day\n",
            "\n",
            "Tokenized(word-level):\n",
            "['everybody', 'dies', 'some', 'day']\n",
            "\n",
            "Sample Stopwords removed:\n",
            "['everybody', 'dies', 'day']\n",
            "\n",
            "Tokens Lemmatized:\n",
            "['everybody', 'dy', 'day']\n",
            "\n",
            "Tokens Stemmed:\n",
            "['everybodi', 'dy', 'day']\n",
            "\n",
            "Encoded Documents:\n",
            "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = fit_vectorizer(TfidfVectorizer, texts=preprocessed_texts)\n",
        "vectorize_texts(vectorizer, texts=preprocessed_texts)\n",
        "vectorize_texts(vectorizer, texts=[preprocess_text(\"Everybody dies some day.\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeJVr8kzEeAW",
        "outputId": "a61fdbdc-79e1-4450-d526-1041c12b63be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizing with <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
            "\n",
            "Vocabulary:\n",
            "{'shall': 14, 'pa': 11, 'axe': 0, 'forget': 6, 'tree': 17, 'rememb': 13, 'dead': 2, 'may': 9, 'never': 10, 'die': 3, 'thou': 16, 'cheat': 1, 'everybodi': 5, 'lie': 7, 'lone': 8, 'wolf': 18, 'dy': 4, 'pack': 12, 'surviv': 15}\n",
            "\n",
            "Encoded Documents:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.77326237\n",
            "  0.         0.         0.6340862  0.         0.         0.\n",
            "  0.        ]\n",
            " [0.5        0.         0.         0.         0.         0.\n",
            "  0.5        0.         0.         0.         0.         0.\n",
            "  0.         0.5        0.         0.         0.         0.5\n",
            "  0.        ]\n",
            " [0.         0.         0.5        0.5        0.         0.\n",
            "  0.         0.         0.         0.5        0.5        0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.61171251 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.50161301 0.         0.61171251 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.70710678\n",
            "  0.         0.70710678 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.4472136  0.\n",
            "  0.         0.         0.4472136  0.         0.         0.\n",
            "  0.4472136  0.         0.         0.4472136  0.         0.\n",
            "  0.4472136 ]]\n",
            "Original:\n",
            "Everybody dies some day.\n",
            "\n",
            "Lowercased:\n",
            "everybody dies some day.\n",
            "\n",
            "Integer Numbers removed:\n",
            "everybody dies some day.\n",
            "\n",
            "Sample Hyperlinks removed:\n",
            "everybody dies some day.\n",
            "\n",
            "Sample Punctuations removed:\n",
            "everybody dies some day\n",
            "\n",
            "Tokenized(word-level):\n",
            "['everybody', 'dies', 'some', 'day']\n",
            "\n",
            "Sample Stopwords removed:\n",
            "['everybody', 'dies', 'day']\n",
            "\n",
            "Tokens Lemmatized:\n",
            "['everybody', 'dy', 'day']\n",
            "\n",
            "Tokens Stemmed:\n",
            "['everybodi', 'dy', 'day']\n",
            "\n",
            "Encoded Documents:\n",
            "[[0.         0.         0.         0.         0.70710678 0.70710678\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.70710678,\n",
              "        0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}